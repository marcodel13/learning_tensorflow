notes:

- every tf script has to respect a general structure: first you define the variables, the operations, the training and prediction phase; then, you run everything in a sessione.

- if you consider a simple feedforward nn (e.g. /Users/marcodeltredici/repository/learning_tensorflow/untitled/TensorFlow-Tutorials-master/04_modern_net.py), you can see the basic elements:

**** 1. you always need to define parameters (weights and bias) as variables (tf.Variable) whit a shape, e.g.:

W = tf.Variable(tf.zeros([784, 10])) # the variable W is a 2-d tensor initialized with all zeros
b = tf.Variable(tf.zeros([10])) # the variable b is a 1-d tensor initialized with all zeros

- you can initialize them also in a different way, e.g.:

W = tf.Variable(tf.random_normal([20, 10], stddev=0.01)) # the variable W is a 2-d tensor initialized with random numbers ina  normal distribution with std 0.01

- note: the shape of the weights matrix is <size of the layer the weight start from> X <size of the layer the weights arrive>. in the example above, the weights directly go from the input layer, whose dimension is 784 features, to the output layer, whose dimension is 10 (and therefore, we have 10 classes).

- The shape of the bias is <size of the layer the weights arrive>

**** 2. you need to define a model, i.e. what is gonna happen in your graph; I think it is better to wrap the model into a function, e.g.:

- feedforward model:

def model(X, w_h, w_h2, w_o, b1, b2, p_keep_input, p_keep_hidden): # note that input X, weights w_h, w_h2, w_o, bias b1 and b2 and the values for dropout have to be defined in point 1

    X = tf.nn.dropout(X, p_keep_input) # modify the input X by multiplying it by the dropout value

    h = tf.nn.relu(tf.matmul(X, w_h) + b1) # the first layer is given by the linear combination of the input and the weight plus the bias, that is later passed through a relu func 
										# note: this step could be decomposed in: h_weighted_activation = tf.matmul(X, w_h) + b1 and then h_output = tf.nn.relu(h_weighted_activation)
    h = tf.nn.dropout(h, p_keep_hidden)

    h2 = tf.nn.relu(tf.matmul(h, w_h2) + b2)

    h2 = tf.nn.dropout(h2, p_keep_hidden)

    return tf.matmul(h2, w_o)
    
*** 3. you need to define the placeholders, i.e. the containers for X and Y that will be taken rom the dataset:

X = tf.placeholder("float", [None, 784]) # X placeholder for a feedforward NN --> None: you don't know the size of the batch; 784: you know each example has 784 features
Y = tf.placeholder("float", [None, 10]) # Y placeholder for a feedforward NN --> None: you don't know the size of the batch; 10: you know the number of classes 

X = tf.placeholder(tf.float32, [None, 20, 1]) # X placeholder for a LSTM --> Number of examples= None: you don't know the size of the batch; Number of inputs = you know each sentence has 20 elements (e.g. 20 words --> padding); Dimension of each input= each input is a single word 
Y = tf.placeholder(tf.float32, [None, 21]) # Y placeholder for a feedforward NN --> None: you don't know the size of the batch; 21: you know the number of classes 
